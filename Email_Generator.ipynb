{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71aXOlO1qoia",
        "outputId": "56b81347-24cc-47f2-89b9-1fed1b4d4e80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers datasets accelerate peft bitsandbytes sentencepiece\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from datasets import load_dataset\n",
        "import torch"
      ],
      "metadata": {
        "id": "ZvWSn6EJziR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Tokenizer"
      ],
      "metadata": {
        "id": "Lx-wyJ4LzmCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n"
      ],
      "metadata": {
        "id": "gcaPAwTJzn-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Dataset"
      ],
      "metadata": {
        "id": "WNKe2I2cztMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"json\", data_files=\"dataset.json\")\n"
      ],
      "metadata": {
        "id": "7DeKNZ1pzulT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess"
      ],
      "metadata": {
        "id": "4jrNI8hfzy1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs, targets = [], []\n",
        "\n",
        "    for c, e in zip(examples[\"context\"], examples[\"email\"]):\n",
        "        if not isinstance(e, dict) or \"body\" not in e:\n",
        "            continue\n",
        "        email_text = e.get(\"subject\", \"\") + \"\\n\" + e[\"body\"]\n",
        "        combined = f\"### Instruction:\\n{c}\\n\\n### Response:\\n{email_text}\"\n",
        "\n",
        "        inputs.append(combined)\n",
        "        targets.append(email_text)\n",
        "\n",
        "    if len(inputs) == 0:\n",
        "        return {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
        "\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        max_length=512,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = dataset[\"train\"].map(preprocess_function, batched=True, remove_columns=[\"context\", \"email\"])"
      ],
      "metadata": {
        "id": "m0qPY7B4z0Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model in 4-bit"
      ],
      "metadata": {
        "id": "JqsxMFjcz_Aw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "XzzMzT1O0Bgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attach LoRA Adapters"
      ],
      "metadata": {
        "id": "vOqyzYec0Okr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "Y5NPe5fo0Tig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Arguments"
      ],
      "metadata": {
        "id": "pKEoqDjs0WDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./mistral-email-lora\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=1,\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    remove_unused_columns=False,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "metadata": {
        "id": "SKs1dIpv0YaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trainer"
      ],
      "metadata": {
        "id": "yj3_ixZ70aSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "xiB66j5O0bPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "sukgCDf30et9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./mistral-email-lora\")\n",
        "\n",
        "print(\"‚úÖ Training completed and adapter saved!\")"
      ],
      "metadata": {
        "id": "h5iCCj610iMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "from transformers import pipeline\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True, device_map=\"auto\")\n",
        "model = PeftModel.from_pretrained(base_model, \"./mistral-email-lora\")\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iaNPEPvj3JxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradio UI"
      ],
      "metadata": {
        "id": "OTmtJBCL2nFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio\n"
      ],
      "metadata": {
        "id": "eePb1ELL2qDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def generate_email(context):\n",
        "    prompt = f\"### Instruction:\\n{context}\\n\\n### Response:\\n\"\n",
        "    out = pipe(prompt, max_new_tokens=250)[0][\"generated_text\"]\n",
        "    return out.split(\"### Response:\")[-1].strip()\n",
        "\n",
        "# Custom CSS for purple, orange, and black theme\n",
        "custom_css = \"\"\"\n",
        "#component-0 {\n",
        "    background: linear-gradient(135deg, #1a0033 0%, #000000 50%, #331a00 100%) !important;\n",
        "}\n",
        ".gradio-container {\n",
        "    font-family: 'Arial', sans-serif !important;\n",
        "}\n",
        "#title {\n",
        "    text-align: center;\n",
        "    background: linear-gradient(90deg, #a855f7, #fb923c);\n",
        "    -webkit-background-clip: text;\n",
        "    -webkit-text-fill-color: transparent;\n",
        "    font-size: 2.5em;\n",
        "    font-weight: bold;\n",
        "    margin-bottom: 10px;\n",
        "}\n",
        ".input-text textarea {\n",
        "    background-color: #1a0033 !important;\n",
        "    border: 2px solid #a855f7 !important;\n",
        "    color: white !important;\n",
        "    min-height: 150px !important;\n",
        "}\n",
        ".output-text textarea {\n",
        "    background-color: #1a0033 !important;\n",
        "    border: 2px solid #fb923c !important;\n",
        "    color: white !important;\n",
        "    min-height: 400px !important;\n",
        "}\n",
        "button {\n",
        "    background: linear-gradient(90deg, #a855f7, #fb923c) !important;\n",
        "    border: none !important;\n",
        "    color: white !important;\n",
        "    font-weight: bold !important;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(css=custom_css) as demo:\n",
        "    gr.Markdown(\"<h1 id='title'>‚ú® Context to Email Generator ‚ú®</h1>\")\n",
        "    gr.Markdown(\"<p style='text-align: center; color: #fb923c;'>Transform your context into professional emails instantly</p>\")\n",
        "\n",
        "    with gr.Row():\n",
        "        context_input = gr.Textbox(\n",
        "            label=\"üìù Context Input\",\n",
        "            placeholder=\"Enter your context here... (e.g., 'Reply to customer complaint about late delivery')\",\n",
        "            lines=8,\n",
        "            elem_classes=\"input-text\"\n",
        "        )\n",
        "\n",
        "    generate_btn = gr.Button(\"üöÄ Generate Email\", size=\"lg\")\n",
        "\n",
        "    with gr.Row():\n",
        "        email_output = gr.Textbox(\n",
        "            label=\"üìß Generated Email\",\n",
        "            lines=20,\n",
        "            elem_classes=\"output-text\"\n",
        "        )\n",
        "\n",
        "    generate_btn.click(fn=generate_email, inputs=context_input, outputs=email_output)\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "g0V386fF2rsy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}